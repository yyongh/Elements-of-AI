## BUILDING AI - MACHINE LEARNING - LINEAR REGRESSION
## Exercise 12: Least squares
## Intermediate
import numpy as np

X = np.array([[66, 5, 15, 2, 500], 
              [21, 3, 50, 1, 100], 
              [120, 15, 5, 2, 1200]])
y = np.array([250000, 60000, 525000])
c = np.array([3000, 200 , -50, 5000, 100])    # coefficient values
 
def squared_error(X, y, c):
    sse = 0.0
    for xi, yi in zip(X, y):
        # add your code here: calculate the predicted price,
        prediction = 0
        for i in range(len(c)):
            prediction += xi[i]*c[i]
        # subtract it from the actual price yi, 
        # square the difference using (yi - prediction)**2,
        se = (yi - prediction)**2
        # and add up all the differences in variable sse
        sse = sse + se
    print(sse)

squared_error(X, y, c)


## Advanced
import numpy as np

# data
X = np.array([[66, 5, 15, 2, 500], 
              [21, 3, 50, 1, 100], 
              [120, 15, 5, 2, 1200]])
y = np.array([250000, 60000, 525000])

# alternative sets of coefficient values
c = np.array([[3000, 200, -50, 5000, 100],  # 1367335000.0
              [2000, -250, -100, 150, 250], # 144765000.0
              [3000, -100, -150, 0, 150]])  # 676665000.0

def find_best(X, y, c):
    smallest_error = np.Inf
    best_index = -1
    for coeff in c:
        # edit here: calculate the sum of squared error with coefficient set coeff and
        # keep track of the one yielding the smallest squared error
        sse = 0.0
        for xi, yi in zip(X, y):
            prediction = 0
            for i in range(len(coeff)):
                prediction += xi[i]*coeff[i]
            se = (yi - prediction)**2
            sse = sse + se

        if smallest_error > sse:
            smallest_error = sse
            best_index += 1

    print("the best set is set %d" % best_index)


find_best(X, y, c)
