## BUILDING AI - MACHINE LEARNING - WORKING WITH TEXT
## Exercise 18: TF-IDF
## Intermediate
# DATA BLOCK

text = '''he really really loves coffee
my sister dislikes coffee
my sister loves tea'''

import math

def main(text):
    # split the text first into lines and then into lists of words
    docs = [line.split() for line in text.splitlines()]

    N = len(docs)

    # create the vocabulary: the list of words that appear at least once
    vocabulary = list(set(text.split()))

    df = {}
    tf = {}
    for word in vocabulary:
        # tf: number of occurrences of word w in document divided by document length
        # note: tf[word] will be a list containing the tf of each word for each document
        # for example tf['he'][0] contains the term frequence of the word 'he' in the first
        # document
        tf[word] = [doc.count(word)/len(doc) for doc in docs]
    
        # df: number of documents containing word w
        df[word] = sum([word in doc for doc in docs])/N
    #print(tf)
    #print(df)

    # loop through documents to calculate the tf-idf values
    for doc_index, doc in enumerate(docs):
        tfidf = []
        for word in vocabulary:
            # ADD THE CORRECT FORMULA HERE. Remember to use the base 10 logarithm: math.log(x, 10)
            tf_idf = tf[word][doc_index] * math.log(1/df[word], 10)
            # tfidf.append(None) 
            tfidf.append(tf_idf)

        print(tfidf)

main(text)


## Advanced *issue at step 4
text = '''Humpty Dumpty sat on a wall
Humpty Dumpty had a great fall
all the king's horses and all the king's men
couldn't put Humpty together again'''

import math

def distance(row1, row2):
    diff = 0
    for i in range(len(row1)):
        diff += abs(row1[i] - row2[i])
    if diff == 0:
        diff = np.inf
    return diff

def all_pairs(data):
    dist = [[distance(sent1, sent2) for sent1 in data] for sent2 in data]
    dist = np.array(dist)
    print(dist)
    #return(dist)

def find_nearest_pair(data):
    N = len(data)
    dist = np.empty((N, N), dtype=float)
    dist = all_pairs(data)
    print(np.unravel_index(np.argmin(dist), dist.shape))

def main(text):
    # tasks your code should perform:

    # 1. split the text into words, and get a list of unique words that appear in it
    # a short one-liner to separate the text into sentences (with words lower-cased to make words equal 
    # despite casing) can be done with 
    # docs = [line.lower().split() for line in text.split('\n')]
    docs = [line.lower().split() for line in text.splitlines()]
    N = len(docs)
    vocabulary = list(set(text.lower().split()))

    # 2. go over each unique word and calculate its term frequency, and its document frequency
    df = {}
    tf = {}
    for word in vocabulary:
        tf[word] = [doc.count(word)/len(doc) for doc in docs]
        df[word] = sum([word in doc for doc in docs])/N
    #print(tf)
    #print(df)

    # 3. after you have your term frequencies and document frequencies, go over each line in the text and 
    # calculate its TF-IDF representation, which will be a vector
    for doc_index, doc in enumerate(docs):
        tfidf = []
        for word in vocabulary:
            tf_idf = tf[word][doc_index] * math.log(1/df[word], 10)
            tfidf.append(tf_idf)
        tfidf = np.array(tfidf)
        print(np.array(tfidf))

    # 4. after you have calculated the TF-IDF representations for each line in the text, you need to
    # calculate the distances between each line to find which are the closest.
    find_nearest_pair(tfidf)
    all_pairs(tfidf)


main(text)

